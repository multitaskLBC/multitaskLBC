{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import definitions of classes and functions for learning by confusion\n",
    "from lbc_utils import *\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set folders of training set and evaluation set\n",
    "TRAIN_FOLDER = 'pictures/tech_train'\n",
    "EVAL_FOLDER = 'pictures/tech_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcab257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training and evaluation (confusion) loop.\n",
    "# the only nonstandard lines are the definitions of Y and Y_bool, which convert the correct label y\n",
    "# to a vector. that vector has the length of the number of categories and each entry corresponds to a\n",
    "# different left-right splitting of the dataset.\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, device='cuda', subset=None):\n",
    "    losses = []\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        Y = lbc_label(y, subset).float()\n",
    "        loss = loss_fn(pred, Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % record_every == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            losses.append(loss)\n",
    "    return losses\n",
    "\n",
    "def confusion_loop(dataloader, model, loss_fn, n_categories, device='cuda', subset=None):\n",
    "    '''\n",
    "    returns running_conf, which is the error p^{err} from Eq. (2) of the article\n",
    "    multiplied by the number of samples per gridpoint. also returns the loss.\n",
    "    '''\n",
    "    torch_weight = confusion_weight(n_categories, subset, device=device).view(1, -1)\n",
    "    running_conf = torch.zeros(n_categories-1, device=device)\n",
    "    running_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            \n",
    "            pred_bool = torch.sigmoid(pred) > 0.5\n",
    "            Y_bool = lbc_label(y, subset)\n",
    "            Y = Y_bool.float()\n",
    "            \n",
    "            confusion = (\n",
    "                1. / (1. - torch_weight) * (pred_bool != Y_bool) * (Y_bool == 1) +\n",
    "                1. / (torch_weight) * (pred_bool != Y_bool) * (Y_bool == 0)\n",
    "            ).sum(0)\n",
    "            \n",
    "            running_conf += confusion\n",
    "            loss = loss_fn(pred, Y)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    return 0.5 * running_conf, running_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d356886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.file_list = [f for f in os.listdir(directory)]\n",
    "        self.pattern = r'technology(-?\\d+)_'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def filename_to_year(self, filename, pattern):  # helper function\n",
    "        match = re.search(pattern, filename)\n",
    "        if match is None:\n",
    "            raise ValueError(f\"could not get year from {filename}\")\n",
    "        return int(match.group(1)) + -1900\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.file_list[index]\n",
    "        \n",
    "        # load image\n",
    "        img_path = os.path.join(self.directory, filename)\n",
    "        with open(img_path, 'rb') as f:\n",
    "            img = Image.open(f).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # load label and convert to tensor\n",
    "        year = self.filename_to_year(filename, self.pattern)\n",
    "        target = torch.tensor(year, dtype=torch.int64)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33899802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymodel(n_categories=130):\n",
    "    '''\n",
    "    load ResNet-50 and replace the final layer for LBC loss.\n",
    "    n_categories: number of categories / grid points\n",
    "    (implies number of grid separators = n_categories - 1)\n",
    "    '''\n",
    "    # Load pretrained model\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    \n",
    "    # Replace the final layer\n",
    "    num_ftrs = model.fc.in_features\n",
    "    n_separators = n_categories - 1\n",
    "    model.fc = torch.nn.Linear(num_ftrs, n_separators)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagnet parameters. preprocess image sizes for faster training.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "dataset = CustomDataset(directory=TRAIN_FOLDER, transform=transform)\n",
    "dataset2 = CustomDataset(directory=EVAL_FOLDER, transform=transform)\n",
    "\n",
    "# set num_workers and persistent_workers for faster dataloaders\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "dataloader2 = torch.utils.data.DataLoader(dataset2, batch_size=32, shuffle=True)\n",
    "\n",
    "n_categories_dataset = 150  # number of categories in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded41ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define other required imports and function definitions here\n",
    "\n",
    "ds_size = len(dataset2)\n",
    "batch_size = 16*64\n",
    "\n",
    "# define training parameters\n",
    "learning_rate = 0.5 * 1e-4\n",
    "epochs = 151\n",
    "record_every = 10\n",
    "subset = list(range(n_categories_dataset - 1))\n",
    "\n",
    "n_categories_total = n_categories_dataset\n",
    "n_categories = len(subset) + 1\n",
    "\n",
    "# initialize model\n",
    "model = mymodel(n_categories).cuda()\n",
    "criterion = LBCWithLogitsLoss(n_categories_dataset, subset, device='cuda')\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "\n",
    "# training and evaluation loop. evaluation here only on training dataset itself for speed.\n",
    "losses = []\n",
    "errs = []\n",
    "valid_losses = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "    # validation loop including confusion signal\n",
    "    conf, valid_loss = confusion_loop(dataloader2, model, criterion, n_categories_dataset, subset=subset)\n",
    "    valid_losses.append(valid_loss)\n",
    "    err = conf.detach().cpu().numpy() / ds_size\n",
    "    errs.append(err)\n",
    "\n",
    "    # plotting\n",
    "    plt.semilogy(subset, err,'-d')\n",
    "    plt.xlabel('system parameter')\n",
    "    plt.ylabel('error')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(valid_losses)\n",
    "    plt.title('vlosses')\n",
    "    plt.show()\n",
    "\n",
    "    # training loop\n",
    "    loss = train_loop(dataloader, model, criterion, optimizer, subset=subset)\n",
    "    losses += loss\n",
    "\n",
    "    # plot train loss\n",
    "    plt.semilogy(losses)\n",
    "    plt.xlabel(f'seen samples [{record_every * batch_size}]')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "\n",
    "# evaluation of results from final loop\n",
    "conf, valid_loss = confusion_loop(dataloader2, model, criterion, n_categories_dataset, subset=subset)\n",
    "valid_losses.append(valid_loss)\n",
    "err = conf.detach().cpu().numpy() / ds_size\n",
    "errs.append(err)\n",
    "\n",
    "# save results\n",
    "results = {'errs': errs, 'losses': losses, 'valid_losses': valid_losses}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
